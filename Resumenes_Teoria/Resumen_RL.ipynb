{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresi√≥n Log√≠stica: Explicaci√≥n Detallada\n",
    "1. Introducci√≥n\n",
    "La regresi√≥n log√≠stica es un modelo estad√≠stico utilizado para la clasificaci√≥n binaria y multiclase. A diferencia de la regresi√≥n lineal, donde la variable dependiente es continua, la regresi√≥n log√≠stica predice probabilidades y se usa para modelar una variable dependiente categ√≥rica.\n",
    "Por ejemplo, en clasificaci√≥n binaria (0 o 1), el modelo estima la probabilidad de que una observaci√≥n pertenezca a una de las dos clases. Si la probabilidad es mayor a un umbral (generalmente 0.5), la predicci√≥n ser√° 1, en caso contrario, ser√° 0.\n",
    "2. Modelo Matem√°tico\n",
    "2.1. Funci√≥n Sigmoide\n",
    "La funci√≥n principal en la regresi√≥n log√≠stica es la sigmoide o log√≠stica, definida como:\n",
    "œÉ(z)=11+e‚àíz\\sigma(z) = \\frac{1}{1 + e^{-z}}œÉ(z)=1+e‚àíz1\n",
    "Donde:\n",
    "‚Ä¢\tz=Œ≤0+Œ≤1x1+Œ≤2x2+...+Œ≤nxnz = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_nz=Œ≤0+Œ≤1x1+Œ≤2x2+...+Œ≤nxn es la combinaci√≥n lineal de los predictores.\n",
    "‚Ä¢\tœÉ(z)\\sigma(z)œÉ(z) transforma cualquier valor real en un rango entre (0,1), interpretado como una probabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_regression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "### CLASIFICADOR ###\n",
    "class VotingEnsembleClassifier:\n",
    "    def __init__(self, voting_type='soft'):\n",
    "        \"\"\"Inicializa el ensemble de clasificadores.\"\"\"\n",
    "        self.voting_clf = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(random_state=42)),\n",
    "                ('rf', RandomForestClassifier(random_state=42)),\n",
    "                ('svc', SVC(probability=True, random_state=42)),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "                ('nb', GaussianNB()),\n",
    "                ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "                ('mlp', MLPClassifier(hidden_layer_sizes=(100,), random_state=42)),\n",
    "                ('et', ExtraTreesClassifier(random_state=42))\n",
    "            ],\n",
    "            voting=voting_type\n",
    "        )\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Entrena el modelo con los datos de entrenamiento.\"\"\"\n",
    "        self.voting_clf.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Realiza predicciones en los datos de prueba.\"\"\"\n",
    "        return self.voting_clf.predict(X_test)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Calcula la precisi√≥n del modelo en los datos de prueba.\"\"\"\n",
    "        y_pred = self.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Precisi√≥n del VotingClassifier: {accuracy:.4f}\")\n",
    "        return accuracy\n",
    "\n",
    "### REGRESOR ###\n",
    "class VotingEnsembleRegressor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializa el ensemble de regresores.\"\"\"\n",
    "        self.voting_reg = VotingRegressor(\n",
    "            estimators=[\n",
    "                ('lr', LinearRegression()),\n",
    "                ('rf', RandomForestRegressor(random_state=42)),\n",
    "                ('svr', SVR()),\n",
    "                ('knn', KNeighborsRegressor(n_neighbors=5)),\n",
    "                ('gb', GradientBoostingRegressor(random_state=42)),\n",
    "                ('mlp', MLPRegressor(hidden_layer_sizes=(100,), random_state=42)),\n",
    "                ('et', ExtraTreesRegressor(random_state=42))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Entrena el modelo con los datos de entrenamiento.\"\"\"\n",
    "        self.voting_reg.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Realiza predicciones en los datos de prueba.\"\"\"\n",
    "        return self.voting_reg.predict(X_test)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Calcula m√©tricas de evaluaci√≥n para regresi√≥n.\"\"\"\n",
    "        y_pred = self.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"Error cuadr√°tico medio (MSE): {mse:.4f}\")\n",
    "        print(f\"Coeficiente de determinaci√≥n (R¬≤): {r2:.4f}\")\n",
    "        return mse, r2\n",
    "\n",
    "# ====== CLASIFICACI√ìN ======\n",
    "print(\"\\n---- CLASIFICACI√ìN ----\")\n",
    "X_class, y_class = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_class, y_class, random_state=42)\n",
    "\n",
    "ensemble_classifier = VotingEnsembleClassifier(voting_type='soft')\n",
    "ensemble_classifier.train(X_train_c, y_train_c)\n",
    "ensemble_classifier.evaluate(X_test_c, y_test_c)\n",
    "\n",
    "# ====== REGRESI√ìN ======\n",
    "print(\"\\n---- REGRESI√ìN ----\")\n",
    "X_reg, y_reg = make_regression(n_samples=500, n_features=5, noise=10, random_state=42)\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, random_state=42)\n",
    "\n",
    "ensemble_regressor = VotingEnsembleRegressor()\n",
    "ensemble_regressor.train(X_train_r, y_train_r)\n",
    "ensemble_regressor.evaluate(X_test_r, y_test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# === 1. DEFINIR VARIABLES ===\n",
    "# Separa las caracter√≠sticas y la variable objetivo\n",
    "X = df_stan.drop(['Precio_euros'], axis=1)\n",
    "y = df_stan['Precio_euros']\n",
    "\n",
    "# Divide los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === 2. ENTRENAR MODELO RANDOM FOREST ===\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones y evaluaci√≥n\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"üîπ Error cuadr√°tico medio (MSE): {mse:.4f}\")\n",
    "print(f\"üîπ Coeficiente de determinaci√≥n (R¬≤): {r2:.4f}\")\n",
    "\n",
    "# === 3. IMPORTANCIA DE CARACTER√çSTICAS CON SHAP ===\n",
    "class FeatureImportanceSHAP:\n",
    "    def __init__(self, model, X_train):\n",
    "        \"\"\"\n",
    "        Clase para calcular la importancia de caracter√≠sticas usando SHAP.\n",
    "        \n",
    "        :param model: Modelo de Machine Learning ya entrenado.\n",
    "        :param X_train: Datos de entrenamiento (sin la variable objetivo).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.explainer = shap.Explainer(model, X_train)\n",
    "        self.shap_values = self.explainer(X_train)\n",
    "\n",
    "    def get_importance(self):\n",
    "        \"\"\"Devuelve un DataFrame con la importancia media de cada caracter√≠stica.\"\"\"\n",
    "        mean_abs_shap = np.abs(self.shap_values.values).mean(axis=0)\n",
    "        feature_names = self.X_train.columns if isinstance(self.X_train, pd.DataFrame) else [f\"Feature {i}\" for i in range(self.X_train.shape[1])]\n",
    "\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'SHAP Importance': mean_abs_shap})\n",
    "        return importance_df.sort_values(by='SHAP Importance', ascending=False)\n",
    "\n",
    "    def plot_summary(self):\n",
    "        \"\"\"Genera el gr√°fico de importancia de caracter√≠sticas con SHAP.\"\"\"\n",
    "        shap.summary_plot(self.shap_values, self.X_train)\n",
    "\n",
    "    def plot_dependence(self, feature_name):\n",
    "        \"\"\"\n",
    "        Genera un gr√°fico de dependencia para una caracter√≠stica espec√≠fica.\n",
    "\n",
    "        :param feature_name: Nombre de la caracter√≠stica a analizar.\n",
    "        \"\"\"\n",
    "        if isinstance(self.X_train, pd.DataFrame):\n",
    "            shap.dependence_plot(feature_name, self.shap_values.values, self.X_train)\n",
    "        else:\n",
    "            print(\"Error: Para graficar dependencia, X_train debe ser un DataFrame con nombres de columnas.\")\n",
    "\n",
    "# === 4. AN√ÅLISIS CON SHAP ===\n",
    "shap_analysis = FeatureImportanceSHAP(rf_model, X_train)\n",
    "\n",
    "# Mostrar la importancia de cada caracter√≠stica\n",
    "print(\"\\nüîπ Importancia de Caracter√≠sticas seg√∫n SHAP:\")\n",
    "print(shap_analysis.get_importance())\n",
    "\n",
    "# Generar gr√°fico SHAP de importancia general\n",
    "shap_analysis.plot_summary()\n",
    "\n",
    "# Graficar la relaci√≥n de una caracter√≠stica espec√≠fica (ejemplo: \"RAM_GB\")\n",
    "shap_analysis.plot_dependence(feature_name=\"RAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
