{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresión Logística: Explicación Detallada\n",
    "1. Introducción\n",
    "La regresión logística es un modelo estadístico utilizado para la clasificación binaria y multiclase. A diferencia de la regresión lineal, donde la variable dependiente es continua, la regresión logística predice probabilidades y se usa para modelar una variable dependiente categórica.\n",
    "Por ejemplo, en clasificación binaria (0 o 1), el modelo estima la probabilidad de que una observación pertenezca a una de las dos clases. Si la probabilidad es mayor a un umbral (generalmente 0.5), la predicción será 1, en caso contrario, será 0.\n",
    "2. Modelo Matemático\n",
    "2.1. Función Sigmoide\n",
    "La función principal en la regresión logística es la sigmoide o logística, definida como:\n",
    "σ(z)=11+e−z\\sigma(z) = \\frac{1}{1 + e^{-z}}σ(z)=1+e−z1\n",
    "Donde:\n",
    "•\tz=β0+β1x1+β2x2+...+βnxnz = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_nz=β0+β1x1+β2x2+...+βnxn es la combinación lineal de los predictores.\n",
    "•\tσ(z)\\sigma(z)σ(z) transforma cualquier valor real en un rango entre (0,1), interpretado como una probabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_regression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "### CLASIFICADOR ###\n",
    "class VotingEnsembleClassifier:\n",
    "    def __init__(self, voting_type='soft'):\n",
    "        \"\"\"Inicializa el ensemble de clasificadores.\"\"\"\n",
    "        self.voting_clf = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(random_state=42)),\n",
    "                ('rf', RandomForestClassifier(random_state=42)),\n",
    "                ('svc', SVC(probability=True, random_state=42)),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "                ('nb', GaussianNB()),\n",
    "                ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "                ('mlp', MLPClassifier(hidden_layer_sizes=(100,), random_state=42)),\n",
    "                ('et', ExtraTreesClassifier(random_state=42))\n",
    "            ],\n",
    "            voting=voting_type\n",
    "        )\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Entrena el modelo con los datos de entrenamiento.\"\"\"\n",
    "        self.voting_clf.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Realiza predicciones en los datos de prueba.\"\"\"\n",
    "        return self.voting_clf.predict(X_test)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Calcula la precisión del modelo en los datos de prueba.\"\"\"\n",
    "        y_pred = self.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Precisión del VotingClassifier: {accuracy:.4f}\")\n",
    "        return accuracy\n",
    "\n",
    "### REGRESOR ###\n",
    "class VotingEnsembleRegressor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializa el ensemble de regresores.\"\"\"\n",
    "        self.voting_reg = VotingRegressor(\n",
    "            estimators=[\n",
    "                ('lr', LinearRegression()),\n",
    "                ('rf', RandomForestRegressor(random_state=42)),\n",
    "                ('svr', SVR()),\n",
    "                ('knn', KNeighborsRegressor(n_neighbors=5)),\n",
    "                ('gb', GradientBoostingRegressor(random_state=42)),\n",
    "                ('mlp', MLPRegressor(hidden_layer_sizes=(100,), random_state=42)),\n",
    "                ('et', ExtraTreesRegressor(random_state=42))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Entrena el modelo con los datos de entrenamiento.\"\"\"\n",
    "        self.voting_reg.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Realiza predicciones en los datos de prueba.\"\"\"\n",
    "        return self.voting_reg.predict(X_test)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Calcula métricas de evaluación para regresión.\"\"\"\n",
    "        y_pred = self.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"Error cuadrático medio (MSE): {mse:.4f}\")\n",
    "        print(f\"Coeficiente de determinación (R²): {r2:.4f}\")\n",
    "        return mse, r2\n",
    "\n",
    "# ====== CLASIFICACIÓN ======\n",
    "print(\"\\n---- CLASIFICACIÓN ----\")\n",
    "X_class, y_class = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_class, y_class, random_state=42)\n",
    "\n",
    "ensemble_classifier = VotingEnsembleClassifier(voting_type='soft')\n",
    "ensemble_classifier.train(X_train_c, y_train_c)\n",
    "ensemble_classifier.evaluate(X_test_c, y_test_c)\n",
    "\n",
    "# ====== REGRESIÓN ======\n",
    "print(\"\\n---- REGRESIÓN ----\")\n",
    "X_reg, y_reg = make_regression(n_samples=500, n_features=5, noise=10, random_state=42)\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, random_state=42)\n",
    "\n",
    "ensemble_regressor = VotingEnsembleRegressor()\n",
    "ensemble_regressor.train(X_train_r, y_train_r)\n",
    "ensemble_regressor.evaluate(X_test_r, y_test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# === 1. DEFINIR VARIABLES ===\n",
    "# Separa las características y la variable objetivo\n",
    "X = df_stan.drop(['Precio_euros'], axis=1)\n",
    "y = df_stan['Precio_euros']\n",
    "\n",
    "# Divide los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === 2. ENTRENAR MODELO RANDOM FOREST ===\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones y evaluación\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"🔹 Error cuadrático medio (MSE): {mse:.4f}\")\n",
    "print(f\"🔹 Coeficiente de determinación (R²): {r2:.4f}\")\n",
    "\n",
    "# === 3. IMPORTANCIA DE CARACTERÍSTICAS CON SHAP ===\n",
    "class FeatureImportanceSHAP:\n",
    "    def __init__(self, model, X_train):\n",
    "        \"\"\"\n",
    "        Clase para calcular la importancia de características usando SHAP.\n",
    "        \n",
    "        :param model: Modelo de Machine Learning ya entrenado.\n",
    "        :param X_train: Datos de entrenamiento (sin la variable objetivo).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.explainer = shap.Explainer(model, X_train)\n",
    "        self.shap_values = self.explainer(X_train)\n",
    "\n",
    "    def get_importance(self):\n",
    "        \"\"\"Devuelve un DataFrame con la importancia media de cada característica.\"\"\"\n",
    "        mean_abs_shap = np.abs(self.shap_values.values).mean(axis=0)\n",
    "        feature_names = self.X_train.columns if isinstance(self.X_train, pd.DataFrame) else [f\"Feature {i}\" for i in range(self.X_train.shape[1])]\n",
    "\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'SHAP Importance': mean_abs_shap})\n",
    "        return importance_df.sort_values(by='SHAP Importance', ascending=False)\n",
    "\n",
    "    def plot_summary(self):\n",
    "        \"\"\"Genera el gráfico de importancia de características con SHAP.\"\"\"\n",
    "        shap.summary_plot(self.shap_values, self.X_train)\n",
    "\n",
    "    def plot_dependence(self, feature_name):\n",
    "        \"\"\"\n",
    "        Genera un gráfico de dependencia para una característica específica.\n",
    "\n",
    "        :param feature_name: Nombre de la característica a analizar.\n",
    "        \"\"\"\n",
    "        if isinstance(self.X_train, pd.DataFrame):\n",
    "            shap.dependence_plot(feature_name, self.shap_values.values, self.X_train)\n",
    "        else:\n",
    "            print(\"Error: Para graficar dependencia, X_train debe ser un DataFrame con nombres de columnas.\")\n",
    "\n",
    "# === 4. ANÁLISIS CON SHAP ===\n",
    "shap_analysis = FeatureImportanceSHAP(rf_model, X_train)\n",
    "\n",
    "# Mostrar la importancia de cada característica\n",
    "print(\"\\n🔹 Importancia de Características según SHAP:\")\n",
    "print(shap_analysis.get_importance())\n",
    "\n",
    "# Generar gráfico SHAP de importancia general\n",
    "shap_analysis.plot_summary()\n",
    "\n",
    "# Graficar la relación de una característica específica (ejemplo: \"RAM_GB\")\n",
    "shap_analysis.plot_dependence(feature_name=\"RAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\denis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, VotingClassifier, VotingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class FeatureImportanceSHAP:\n",
    "    def __init__(self, model, X_train):\n",
    "        \"\"\"\n",
    "        Clase para calcular la importancia de características usando SHAP.\n",
    "        \n",
    "        :param model: Modelo de Machine Learning ya entrenado.\n",
    "        :param X_train: Datos de entrenamiento (sin la variable objetivo).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.explainer = shap.Explainer(model, X_train)\n",
    "        self.shap_values = self.explainer(X_train)\n",
    "\n",
    "    def get_importance(self):\n",
    "        \"\"\"Devuelve un DataFrame con la importancia media de cada característica.\"\"\"\n",
    "        mean_abs_shap = np.abs(self.shap_values.values).mean(axis=0)\n",
    "        feature_names = self.X_train.columns if isinstance(self.X_train, pd.DataFrame) else [f\"Feature {i}\" for i in range(self.X_train.shape[1])]\n",
    "\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'SHAP Importance': mean_abs_shap})\n",
    "        return importance_df.sort_values(by='SHAP Importance', ascending=False)\n",
    "\n",
    "    def plot_summary(self):\n",
    "        \"\"\"Genera el gráfico de importancia de características con SHAP.\"\"\"\n",
    "        shap.summary_plot(self.shap_values, self.X_train)\n",
    "\n",
    "    def plot_dependence(self, feature_name):\n",
    "        \"\"\"\n",
    "        Genera un gráfico de dependencia para una característica específica.\n",
    "\n",
    "        :param feature_name: Nombre de la característica a analizar.\n",
    "        \"\"\"\n",
    "        if isinstance(self.X_train, pd.DataFrame):\n",
    "            shap.dependence_plot(feature_name, self.shap_values.values, self.X_train)\n",
    "        else:\n",
    "            print(\"Error: Para graficar dependencia, X_train debe ser un DataFrame con nombres de columnas.\")\n",
    "\n",
    "class VotingEnsembleClassifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"Clase para entrenar un Voting Classifier (para clasificación).\"\"\"\n",
    "        self.model = VotingClassifier(estimators=[\n",
    "            ('lr', LogisticRegression(random_state=42)),\n",
    "            ('rf', RandomForestClassifier(random_state=42)),\n",
    "            ('svc', SVC(probability=True, random_state=42))\n",
    "        ], voting='soft')\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Entrena el modelo.\"\"\"\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evalúa el modelo con accuracy.\"\"\"\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"🔹 Accuracy: {accuracy:.4f}\")\n",
    "        return accuracy\n",
    "\n",
    "    def feature_importance(self, X_train):\n",
    "        \"\"\"Calcula la importancia de características usando SHAP.\"\"\"\n",
    "        shap_analysis = FeatureImportanceSHAP(self.model, X_train)\n",
    "        print(\"\\n🔹 Importancia de Características (SHAP):\")\n",
    "        print(shap_analysis.get_importance())\n",
    "        shap_analysis.plot_summary()\n",
    "\n",
    "class VotingEnsembleRegressor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Clase para entrenar un Voting Regressor (para regresión).\"\"\"\n",
    "        self.model = VotingRegressor(estimators=[\n",
    "            ('rf', RandomForestRegressor(random_state=42)),\n",
    "        ])\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Entrena el modelo.\"\"\"\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evalúa el modelo con MSE y R².\"\"\"\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"🔹 MSE: {mse:.4f}\")\n",
    "        print(f\"🔹 R²: {r2:.4f}\")\n",
    "        return mse, r2\n",
    "\n",
    "    def feature_importance(self, X_train):\n",
    "        \"\"\"Calcula la importancia de características usando SHAP.\"\"\"\n",
    "        shap_analysis = FeatureImportanceSHAP(self.model, X_train)\n",
    "        print(\"\\n🔹 Importancia de Características (SHAP):\")\n",
    "        print(shap_analysis.get_importance())\n",
    "        shap_analysis.plot_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que `y_class` es la variable categórica que queremos predecir\n",
    "X = df_stan.drop(['Categoria_Laptop'], axis=1)\n",
    "y = df_stan['Categoria_Laptop']\n",
    "\n",
    "# Convertir la variable categórica a numérica si es necesario\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# División de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar modelo de clasificación\n",
    "classifier = VotingEnsembleClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.evaluate(X_test, y_test)\n",
    "classifier.feature_importance(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
